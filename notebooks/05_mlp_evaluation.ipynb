{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: MLP Evaluation and Internal Statistical Analysis\n",
    "\n",
    "This notebook evaluates the Multi-layer Perceptron (MLP) classifier on the 12 best-performing datasets. It then performs an internal statistical analysis to compare the different hyperparameter combinations for the `MLPClassifier`.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Load Best Datasets**: The list of 12 dataset filenames is loaded from `../results/top_12_datasets.joblib`.\n",
    "2.  **Run MLP Evaluation**: For each of the top datasets, `GridSearchCV` is used to find the best hyperparameters for the `MLPClassifier`.\n",
    "3.  **Internal Statistical Analysis**: Friedman and Nemenyi tests are performed to compare the performance of different hyperparameter combinations *within* the MLP classifier on the top 12 datasets.\n",
    "4.  **Save Results**: The detailed MLP evaluation results (including all hyperparameter combinations) are saved to `../results/mlp_full_results.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "RESULTS_DIR = '../results/'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "BEST_DATASETS_PATH = os.path.join(RESULTS_DIR, 'top_12_datasets.joblib')\n",
    "\n",
    "# Load the list of best datasets\n",
    "try:\n",
    "    BEST_DATASETS = joblib.load(BEST_DATASETS_PATH)\n",
    "    print(f'Successfully loaded {len(BEST_DATASETS)} best datasets.')\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The file {BEST_DATASETS_PATH} was not found.')\n",
    "    print('Please run Notebook 02 first to generate the best datasets list.')\n",
    "    BEST_DATASETS = []\n",
    "\n",
    "mlp_grid_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (120,), (150,), (70,70), (100,50)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'max_iter': [500, 1000, 1500, 2000] # Reduced max_iter to speed up, can be increased\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mlp_results = [] # To store all parameter combinations for statistical analysis\n",
    "best_mlp_per_dataset = [] # To store only the best MLP for each dataset\n",
    "\n",
    "if BEST_DATASETS:\n",
    "    for dataset_name in BEST_DATASETS:\n",
    "        start_time = time.time()\n",
    "        print(f'Evaluating MLP on {dataset_name}...')\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, dataset_name))\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "\n",
    "        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(MLPClassifier(random_state=42), mlp_grid_params, cv=kf, scoring='f1_weighted', n_jobs=-1, return_train_score=False)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Store all results from GridSearchCV for internal statistical analysis\n",
    "        for i, params in enumerate(grid_search.cv_results_['params']:\n",
    "            full_mlp_results.append({\n",
    "                'dataset': dataset_name,\n",
    "                'classifier': 'MLP',\n",
    "                'hidden_layer_sizes': str(params['hidden_layer_sizes']),\n",
    "                'activation': params['activation'],\n",
    "                'solver': params['solver'],\n",
    "                'learning_rate_init': params['learning_rate_init'],\n",
    "                'max_iter': params['max_iter'],\n",
    "                'f1_cv_mean': grid_search.cv_results_['mean_test_score'][i],\n",
    "                'f1_cv_std': grid_search.cv_results_['std_test_score'][i],\n",
    "                'duration_seconds': duration \n",
    "            })\n",
    "\n",
    "        # Store only the best result for this dataset for overall comparison later\n",
    "        best_mlp_per_dataset.append({\n",
    "            'dataset': dataset_name,\n",
    "            'classifier': 'MLP',\n",
    "            'f1_cv_mean': grid_search.best_score_,
",
    "            'best_params': str(grid_search.best_params_),\n",
    "            'duration_seconds': duration\n",
    "        })\n",
    "\n",
    "        print(f"  Best F1-score: {grid_search.best_score_:.4f} with params {grid_search.best_params_} (took {duration:.2f}s)")\n",
    "\n",
    "    # --- Save the full results ---\n",
    "    full_mlp_results_df = pd.DataFrame(full_mlp_results)\n",
    "    full_mlp_results_df.to_csv(os.path.join(RESULTS_DIR, 'mlp_full_results.csv'), index=False)\n",
    "\n",
    "    # Save the best results for each dataset for final comparison\n",
    "    best_mlp_df = pd.DataFrame(best_mlp_per_dataset)\n",
    "    best_mlp_df.to_csv(os.path.join(RESULTS_DIR, 'mlp_best_scores.csv'), index=False)\n",
    "\n",
    "    print('\nSaved full MLP evaluation results to `mlp_full_results.csv`')\n",
    "    print('Saved best MLP scores per dataset to `mlp_best_scores.csv`')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Internal Statistical Analysis of MLP Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_DATASETS:\n",
    "    print('\nPerforming internal statistical analysis for MLP hyperparameters on the top 12 datasets...')\n",
    "\n",
    "    # Create a unique identifier for each hyperparameter combination\n",
    "    full_mlp_results_df['param_combo'] = full_mlp_results_df.apply(\n",
    "        lambda row: f"hls={row['hidden_layer_sizes']},act={row['activation']},solv={row['solver']},lr={row['learning_rate_init']},max_iter={row['max_iter']}", axis=1\n",
    "    )\n",
    "\n",
    "    # Create a pivot table for Friedman test: datasets as rows, param_combo as columns, f1_cv_mean as values\n",
    "    pivot_mlp_stats = full_mlp_results_df.pivot_table(\n",
    "        index='dataset', \n",
    "        columns='param_combo', \n",
    "        values='f1_cv_mean'\n",
    "    )\n",
    "\n",
    "    print("MLP F1-scores for different hyperparameter combinations on Top 12 Datasets:")\n",
    "    display(pivot_mlp_stats)\n",
    "\n",
    "    # --- Friedman Test ---\n",
    "    stat, p_value = friedmanchisquare(*[pivot_mlp_stats[col] for col in pivot_mlp_stats.columns])\n",
    "\n",
    "    print(f'\nFriedman Test Results for MLP Hyperparameters:')\n",
    "    print(f'  - Statistic: {stat:.4f}')\n",
    "    print(f'  - p-value: {p_value:.4f}')\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print('\n  Conclusion: There is a statistically significant difference between different MLP hyperparameter combinations.')\n",
    "        \n",
    "        # --- Nemenyi Post-hoc Test ---\n",
    "        nemenyi_results = sp.posthoc_nemenyi_friedman(pivot_mlp_stats.T)\n",
    "        \n",
    "        print('\nNemenyi Post-hoc Test Results (p-values for hyperparameter comparison):')\n",
    "        display(nemenyi_results)\n",
    "        \n",
    "        # --- Visualize Nemenyi Results ---\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sp.sign_plot(nemenyi_results, **{'style': 'default'})\n",
    "        plt.title('Nemenyi Test Critical Difference Plot for MLP Hyperparameters')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print('\n  Conclusion: There is no statistically significant difference between different MLP hyperparameter combinations.')\n",
    "else:\n",
    "    print('Cannot perform statistical analysis without BEST_DATASETS. Please run Notebook 02 first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Overall Top MLP Performing Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_DATASETS:\n",
    "    display(best_mlp_df.sort_values(by='f1_cv_mean', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
