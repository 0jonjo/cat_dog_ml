{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Feature Extraction and Dimensionality Reduction\n",
    "\n",
    "This notebook performs the first two major steps of the image classification pipeline:\n",
    "\n",
    "1.  **Feature Extraction**: It reads the raw images and applies two different visual descriptors to extract features:\n",
    "    *   **Local Binary Patterns (LBP)**: Captures texture information.\n",
    "    *   **Histogram of Oriented Gradients (HOG)**: Captures shape and edge information.\n",
    "2.  **Dimensionality Reduction**: It applies **Principal Component Analysis (PCA)** to the generated feature sets to reduce their dimensionality.\n",
    "\n",
    "All resulting datasets are saved as `.csv` files in the `../results/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image directory: /home/jonjo/Code/jonjo/cat_dog_ml/data/images\n",
      "Results directory: /home/jonjo/Code/jonjo/cat_dog_ml/results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "IMAGE_DIR = '../data/images/'\n",
    "RESULTS_DIR = '../results/'\n",
    "\n",
    "# Ensure the results directory exists\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "\n",
    "# Image and feature parameters from the paper\n",
    "IMAGE_SIZES = [128, 256]\n",
    "LBP_RADIUS_OPTIONS = [3, 6, 12]\n",
    "HOG_PIXELS_PER_CELL_OPTIONS = [(8, 8), (16, 16), (20, 20), (32, 32)]\n",
    "PCA_VAR_THRESHOLDS = [0.90, 0.75]\n",
    "\n",
    "print(f'Image directory: {os.path.abspath(IMAGE_DIR)}')\n",
    "print(f'Results directory: {os.path.abspath(RESULTS_DIR)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Loading and Preprocessing\n",
    "\n",
    "This function loads all images from the specified directory, resizes them, and assigns labels based on their parent folder name. The paper defines 'cat' classes (Birman, Ragdoll) as label 1 and 'dog' classes (Miniature Pinscher, English Setter) as label 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_dir, image_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "\n",
    "    for class_name in ['dogs', 'cats']:\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            print(f\"Warning: Directory {class_dir} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        label = 0 if class_name == 'dogs' else 1 # 0 for dogs, 1 for cats\n",
    "\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(class_dir, filename)\n",
    "                image = imread(image_path)\n",
    "                image_resized = resize(image, (image_size, image_size))\n",
    "                images.append(image_resized)\n",
    "                labels.append(label)\n",
    "                filenames.append(filename)\n",
    "\n",
    "    print(f'Loaded {len(images)} images of size {image_size}x{image_size}.')\n",
    "    return images, labels, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LBP Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lbp_features(images, radius):\n",
    "    lbp_features_list = []\n",
    "    n_points = 8 * radius\n",
    "\n",
    "    for img in images:\n",
    "        img_gray = rgb2gray(img)\n",
    "        lbp = local_binary_pattern(img_gray, n_points, radius, method='uniform')\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "        lbp_features_list.append(hist)\n",
    "\n",
    "    return np.array(lbp_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HOG Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(images, pixels_per_cell):\n",
    "    hog_features_list = []\n",
    "\n",
    "    for img in images:\n",
    "        # The HOG descriptor is multichannel-aware, no need for rgb2gray if image has 3 channels\n",
    "        fd = hog(img, orientations=9, pixels_per_cell=pixels_per_cell,\n",
    "                 cells_per_block=(2, 2), visualize=False, channel_axis=-1) # Use channel_axis for color images\n",
    "        hog_features_list.append(fd)\n",
    "\n",
    "    return np.array(hog_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate and Save Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in IMAGE_SIZES:\n",
    "    images, labels, _ = load_images(IMAGE_DIR, size)\n",
    "\n",
    "    # Generate LBP datasets\n",
    "    for radius in LBP_RADIUS_OPTIONS:\n",
    "        print(f'Generating LBP dataset for image size {size} and radius {radius}...')\n",
    "        features = extract_lbp_features(images, radius)\n",
    "        df = pd.DataFrame(features)\n",
    "        df['label'] = labels\n",
    "        filename = f'LBP_{size}_{radius}r.csv'\n",
    "        df.to_csv(os.path.join(RESULTS_DIR, filename), index=False)\n",
    "        print(f'Saved {filename}')\n",
    "\n",
    "    # Generate HOG datasets\n",
    "    for ppc in HOG_PIXELS_PER_CELL_OPTIONS:\n",
    "        print(f'Generating HOG dataset for image size {size} and pixels per cell {ppc}...')\n",
    "        features = extract_hog_features(images, ppc)\n",
    "        df = pd.DataFrame(features)\n",
    "        df['label'] = labels\n",
    "        filename = f'HOG_{size}_{ppc[0]}x{ppc[1]}.csv'\n",
    "        df.to_csv(os.path.join(RESULTS_DIR, filename), index=False)\n",
    "        print(f'Saved {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply PCA to Generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir(RESULTS_DIR) if f.endswith('.csv') and 'PCA' not in f]\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f'Applying PCA to {file}...')\n",
    "    dataset = pd.read_csv(os.path.join(RESULTS_DIR, file))\n",
    "\n",
    "    X = dataset.iloc[:, :-1] # Features\n",
    "    y = dataset.iloc[:, -1]  # Target\n",
    "\n",
    "    # Standardize the features before PCA\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "    for var_thresh in PCA_VAR_THRESHOLDS:\n",
    "        pca = PCA(n_components=var_thresh, whiten=True)\n",
    "        pca_result = pca.fit_transform(X_std)\n",
    "\n",
    "        print(f'  - PCA with {var_thresh*100}% variance retained {X.shape[1]} features into {pca.n_components_}.')\n",
    "\n",
    "        # Create a new DataFrame with the PCA results\n",
    "        pca_cols = [f'pca_{i+1}' for i in range(pca_result.shape[1])]\n",
    "        pca_df = pd.DataFrame(data=pca_result, columns=pca_cols)\n",
    "        final_df = pca_df.join(y)\n",
    "\n",
    "        # Save the new dataset\n",
    "        file_without_ext = os.path.splitext(file)[0]\n",
    "        output_filename = f'{file_without_ext}_PCA-{int(var_thresh*100)}.csv'\n",
    "        final_df.to_csv(os.path.join(RESULTS_DIR, output_filename), index=False)\n",
    "        print(f'  - Saved {output_filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
