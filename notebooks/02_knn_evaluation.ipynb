{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: k-NN Evaluation and Internal Statistical Analysis\n",
    "\n",
    "This notebook evaluates the k-Nearest Neighbors (k-NN) classifier on all datasets generated by Notebook 1. It identifies the 12 best-performing feature representations based on k-NN's performance and then performs an internal statistical analysis to compare the different `n_neighbors` (k) values.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Load Datasets**: All `.csv` files from the `../results/` directory are loaded.\n",
    "2.  **Run k-NN Evaluation**: For each dataset, `GridSearchCV` is used to find the best `k` (from 1 to 15) using 10-fold cross-validation.\n",
    "3.  **Identify Top Datasets**: The datasets are ranked by their mean cross-validated F1 score, and the top 12 are selected.\n",
    "4.  **Internal Statistical Analysis**: Friedman and Nemenyi tests are performed to compare the performance of different `n_neighbors` values *within* the k-NN classifier on the top 12 datasets.\n",
    "5.  **Save Results**: The list of the top 12 dataset filenames is saved to `top_12_datasets.joblib`, and the detailed k-NN evaluation results (including all `k` values) are saved to `knn_full_results.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "RESULTS_DIR = '../results/'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "ALL_DATASETS = sorted([f for f in os.listdir(RESULTS_DIR) if f.endswith('.csv') and 'full_results' not in f and 'scores' not in f and 'top_12_datasets' not in f])\n",
    "TOP_N_DATASETS = 12\n",
    "\n",
    "print(f'Found {len(ALL_DATASETS)} datasets to process.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate k-NN on all Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_knn_results = [] # To store all parameter combinations for statistical analysis\n",
    "best_knn_per_dataset = [] # To store only the best k for each dataset\n",
    "\n",
    "knn_grid_params = {'n_neighbors': list(range(1, 16))}\n",
    "\n",
    "for dataset_name in ALL_DATASETS:\n",
    "    start_time = time.time()\n",
    "    print(f'Evaluating k-NN on {dataset_name}...')\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(RESULTS_DIR, dataset_name))\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), knn_grid_params, cv=kf, scoring='f1_weighted', n_jobs=-1, return_train_score=False)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    # Store all results from GridSearchCV for internal statistical analysis\n",
    "    for i, params in enumerate(grid_search.cv_results_['params']:\n",
    "        full_knn_results.append({\n",
    "            'dataset': dataset_name,\n",
    "            'classifier': 'kNN',\n",
    "            'n_neighbors': params['n_neighbors'],\n",
    "            'f1_cv_mean': grid_search.cv_results_['mean_test_score'][i],\n",
    "            'f1_cv_std': grid_search.cv_results_['std_test_score'][i],\n",
    "            'duration_seconds': duration # Total duration for dataset, not per param\n",
    "        })\n",
    "\n",
    "    # Store only the best result for this dataset for overall comparison later\n",
    "    best_knn_per_dataset.append({\n",
    "        'dataset': dataset_name,\n",
    "        'classifier': 'kNN',\n",
    "        'f1_cv_mean': grid_search.best_score_,",
    "        'best_params': str(grid_search.best_params_),\n",
    "        'duration_seconds': duration\n",
    "    })\n",
    "\n",
    "    print(f"  Best F1-score: {grid_search.best_score_:.4f} with params {grid_search.best_params_} (took {duration:.2f}s)")\n",
    "\n",
    "# --- Identify and Save the Top 12 Datasets based on best k-NN performance ---\n",
    "best_knn_df = pd.DataFrame(best_knn_per_dataset)\n",
    "top_12_df = best_knn_df.nlargest(TOP_N_DATASETS, 'f1_cv_mean')\n",
    "\n",
    "best_dataset_names = top_12_df['dataset'].tolist()\n",
    "\n",
    "# Save the list of best dataset names for other notebooks to use\n",
    "joblib.dump(best_dataset_names, os.path.join(RESULTS_DIR, 'top_12_datasets.joblib'))\n",
    "\n",
    "# Save the full k-NN results for statistical analysis (internal and external)\n",
    "full_knn_results_df = pd.DataFrame(full_knn_results)\n",
    "full_knn_results_df.to_csv(os.path.join(RESULTS_DIR, 'knn_full_results.csv'), index=False)\n",
    "\n",
    "print(f'\nSaved the list of {len(best_dataset_names)} best performing datasets to `top_12_datasets.joblib`')\n",
    "print('Saved full k-NN evaluation results to `knn_full_results.csv` (includes all k values)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Internal Statistical Analysis of k-NN (`n_neighbors`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\nPerforming internal statistical analysis for k-NN hyperparameters on the top 12 datasets...')\n",
    "\n",
    "# Filter results to only include the top 12 datasets\n",
    "knn_filtered_for_stats = full_knn_results_df[full_knn_results_df['dataset'].isin(best_dataset_names)]\n",
    "\n",
    "# Create a pivot table for Friedman test: datasets as rows, n_neighbors as columns, f1_cv_mean as values\n",
    "pivot_knn_stats = knn_filtered_for_stats.pivot_table(\n",
    "    index='dataset', \n",
    "    columns='n_neighbors', \n",
    "    values='f1_cv_mean'\n",
    ")\n",
    "\n",
    "print(\"k-NN F1-scores for different 'n_neighbors' on Top 12 Datasets:\")\n",
    "display(pivot_knn_stats)\n",
    "\n",
    "# --- Friedman Test ---\n",
    "stat, p_value = friedmanchisquare(*[pivot_knn_stats[col] for col in pivot_knn_stats.columns])\n",
    "\n",
    "print(f'\nFriedman Test Results for k-NN n_neighbors:')\n",
    "print(f'  - Statistic: {stat:.4f}')\n",
    "print(f'  - p-value: {p_value:.4f}')\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print('\n  Conclusion: There is a statistically significant difference between different `n_neighbors` values.')\n",
    "    \n",
    "    # --- Nemenyi Post-hoc Test ---\n",
    "    nemenyi_results = sp.posthoc_nemenyi_friedman(pivot_knn_stats.T)\n",
    "    \n",
    "    print('\nNemenyi Post-hoc Test Results (p-values for n_neighbors comparison):')\n",
    "    display(nemenyi_results)\n",
    "    \n",
    "    # --- Visualize Nemenyi Results ---\n",
    "    sp.sign_plot(nemenyi_results, **{'style': 'default'})\n",
    "    plt.title('Nemenyi Test Critical Difference Plot for k-NN (n_neighbors)')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print('\n  Conclusion: There is no statistically significant difference between different `n_neighbors` values.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Overall Top k-NN Performing Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Top {TOP_N_DATASETS} datasets based on best k-NN F1-score:')\n",
    "display(top_12_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}