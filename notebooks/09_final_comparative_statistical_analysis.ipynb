{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 9: Final Comparative Statistical Analysis\n",
    "\n",
    "This notebook performs a final statistical comparison *between* the best performing configurations of all the evaluated classifiers across the 12 best datasets.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Load Best Scores**: Loads the best F1 scores for k-NN, Decision Tree, Random Forest, MLP, Naive Bayes, VotingClassifier, and StackingClassifier.\n",
    "2.  **Combine and Prepare Data**: Consolidates all best scores into a single DataFrame and creates a pivot table.\n",
    "3.  **Perform Statistical Tests**: Runs the Friedman test to check for overall significant differences, followed by the Nemenyi post-hoc test to identify specific differences between classifier pairs.\n",
    "4.  **Visualize Results**: Displays the pivot table, test results, and a critical difference plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---"",
    "RESULTS_DIR = '../results/'\n",
    "BEST_DATASETS_PATH = os.path.join(RESULTS_DIR, 'top_12_datasets.joblib')\n",
    "\n",
    "FILE_PATHS = [\n",
    "    os.path.join(RESULTS_DIR, 'knn_full_results.csv'), # Best k-NN scores are extracted from here\n",
    "    os.path.join(RESULTS_DIR, 'decision_tree_best_scores.csv'),\n",
    "    os.path.join(RESULTS_DIR, 'random_forest_best_scores.csv'),\n",
    "    os.path.join(RESULTS_DIR, 'mlp_best_scores.csv'),\n",
    "    os.path.join(RESULTS_DIR, 'naive_bayes_best_scores.csv'),\n",
    "    os.path.join(RESULTS_DIR, 'voting_classifier_best_scores.csv'),\n",
    "    os.path.join(RESULTS_DIR, 'stacking_classifier_best_scores.csv')\n",
    "]\n",
    "\n",
    "all_best_scores = []\n",
    "try:\n",
    "    BEST_DATASETS = joblib.load(BEST_DATASETS_PATH)\n",
    "    for path in FILE_PATHS:\n",
    "        if 'knn_full_results.csv' in path: # Extract best k-NN scores from full results\n",
    "            knn_full = pd.read_csv(path)\n",
    "            best_knn_per_dataset = knn_full.loc[knn_full.groupby('dataset')['f1_cv_mean'].idxmax()].copy()\n",
    "            best_knn_per_dataset['classifier'] = 'kNN'\n",
    "            best_knn_per_dataset['best_params'] = best_knn_per_dataset['n_neighbors'].apply(lambda x: f"{{'n_neighbors': {x}}}")\n",
    "            all_best_scores.append(best_knn_per_dataset[['dataset', 'classifier', 'f1_cv_mean', 'best_params']])\n",
    "        else:\n",
    "            all_best_scores.append(pd.read_csv(path))\n",
    "    print('Successfully loaded all best classifier scores and dataset list.')\n",
    "except FileNotFoundError as e:\n",
    "    print(f'Error: {e.filename} not found.')\n",
    "    print('Please run all preceding notebooks first to generate the necessary score files.')\n",
    "    BEST_DATASETS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combine and Prepare Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_DATASETS and all_best_scores:\n",
    "    # Concatenate all best scores dataframes\n",
    "    combined_best_scores = pd.concat(all_best_scores)\n",
    "\n",
    "    # Filter to include only the top 12 datasets (should already be the case if previous notebooks ran correctly)\n",
    "    filtered_scores = combined_best_scores[combined_best_scores['dataset'].isin(BEST_DATASETS)]\n",
    "\n",
    "    # Create the pivot table for statistical tests\n",
    "    pivot_final_stats = filtered_scores.pivot_table(\n",
    "        index='dataset', \n",
    "        columns='classifier', \n",
    "        values='f1_cv_mean'\n",
    "    )\n",
    "    # Ensure all classifiers are present as columns, fill NaNs if any dataset missed a classifier (shouldn't happen)\n",
    "    pivot_final_stats = pivot_final_stats.fillna(0) # Filling NaNs with 0, consider average or other strategy if NaNs expected\n",
    "\n",
    "    print(\"Best Classifier Performance on Top 12 Datasets:")\n",
    "    display(pivot_final_stats)\n",
    "else:\n",
    "    print('Cannot perform final statistical analysis without necessary data. Please check previous notebook runs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pivot_final_stats' in locals() and not pivot_final_stats.empty:\n",
    "    # --- Friedman Test ---"",
    "    stat, p_value = friedmanchisquare(*[pivot_final_stats[col] for col in pivot_final_stats.columns])\n",
    "\n",
    "    print(f'\nFriedman Test Results for Best Classifiers:')\n",
    "    print(f'  - Statistic: {stat:.4f}')\n",
    "    print(f'  - p-value: {p_value:.4f}')\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print('\n  Conclusion: There is a statistically significant difference between the best performing classifiers.')\n",
    "        \n",
    "        # --- Nemenyi Post-hoc Test ---"",
    "        nemenyi_results = sp.posthoc_nemenyi_friedman(pivot_final_stats.T)\n",
    "        \n",
    "        print('\nNemenyi Post-hoc Test Results (p-values for classifier comparison):')\n",
    "        display(nemenyi_results)\n",
    "        \n",
    "        # --- Visualize Nemenyi Results ---"",
    "        plt.figure(figsize=(10, 8))\n",
    "        sp.sign_plot(nemenyi_results, **{'style': 'default'})\n",
    "        plt.title('Nemenyi Test Critical Difference Plot for Best Classifiers')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print('\n  Conclusion: There is no statistically significant difference between the best performing classifiers.')\n",
    "else:\n",
    "    print('Cannot perform statistical analysis. Pivot table is empty or not created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
