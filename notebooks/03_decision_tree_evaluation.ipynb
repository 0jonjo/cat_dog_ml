{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Decision Tree Evaluation and Internal Statistical Analysis\n",
    "\n",
    "This notebook evaluates the Decision Tree classifier on the 12 best-performing datasets identified in Notebook 2. It then performs an internal statistical analysis to compare the different hyperparameter combinations for the `DecisionTreeClassifier`.\n",
    "\n",
    "The process is as follows:\n",
    "1.  **Load Best Datasets**: The list of 12 dataset filenames is loaded from `../results/top_12_datasets.joblib`.\n",
    "2.  **Run Decision Tree Evaluation**: For each of the top datasets, `GridSearchCV` is used to find the best hyperparameters for the `DecisionTreeClassifier`.\n",
    "3.  **Internal Statistical Analysis**: Friedman and Nemenyi tests are performed to compare the performance of different hyperparameter combinations *within* the Decision Tree classifier on the top 12 datasets.\n",
    "4.  **Save Results**: The detailed Decision Tree evaluation results (including all hyperparameter combinations) are saved to `../results/decision_tree_full_results.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "RESULTS_DIR = '../results/'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "BEST_DATASETS_PATH = os.path.join(RESULTS_DIR, 'top_12_datasets.joblib')\n",
    "\n",
    "# Load the list of best datasets\n",
    "try:\n",
    "    BEST_DATASETS = joblib.load(BEST_DATASETS_PATH)\n",
    "    print(f'Successfully loaded {len(BEST_DATASETS)} best datasets.')\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: The file {BEST_DATASETS_PATH} was not found.')\n",
    "    print('Please run Notebook 02 first to generate the best datasets list.')\n",
    "    BEST_DATASETS = []\n",
    "\n",
    "dt_grid_params = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': list(range(2, 16)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dt_results = [] # To store all parameter combinations for statistical analysis\n",
    "best_dt_per_dataset = [] # To store only the best DT for each dataset\n",
    "\n",
    "if BEST_DATASETS:\n",
    "    for dataset_name in BEST_DATASETS:\n",
    "        start_time = time.time()\n",
    "        print(f'Evaluating Decision Tree on {dataset_name}...')\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, dataset_name))\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "\n",
    "        X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_grid_params, cv=kf, scoring='f1_weighted', n_jobs=-1, return_train_score=False)\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Store all results from GridSearchCV for internal statistical analysis\n",
    "        for i, params in enumerate(grid_search.cv_results_['params']:\n",
    "            full_dt_results.append({\n",
    "                'dataset': dataset_name,\n",
    "                'classifier': 'DecisionTree',\n",
    "                'criterion': params['criterion'],\n",
    "                'max_depth': params['max_depth'],\n",
    "                'f1_cv_mean': grid_search.cv_results_['mean_test_score'][i],\n",
    "                'f1_cv_std': grid_search.cv_results_['std_test_score'][i],\n",
    "                'duration_seconds': duration \n",
    "            })\n",
    "\n",
    "        # Store only the best result for this dataset for overall comparison later\n",
    "        best_dt_per_dataset.append({\n",
    "            'dataset': dataset_name,\n",
    "            'classifier': 'DecisionTree',\n",
    "            'f1_cv_mean': grid_search.best_score_,
",
    "            'best_params': str(grid_search.best_params_),\n",
    "            'duration_seconds': duration\n",
    "        })\n",
    "\n",
    "        print(f"  Best F1-score: {grid_search.best_score_:.4f} with params {grid_search.best_params_} (took {duration:.2f}s)")\n",
    "\n",
    "    # --- Save the full results ---\n",
    "    full_dt_results_df = pd.DataFrame(full_dt_results)\n",
    "    full_dt_results_df.to_csv(os.path.join(RESULTS_DIR, 'decision_tree_full_results.csv'), index=False)\n",
    "\n",
    "    # Save the best results for each dataset for final comparison\n",
    "    best_dt_df = pd.DataFrame(best_dt_per_dataset)\n",
    "    best_dt_df.to_csv(os.path.join(RESULTS_DIR, 'decision_tree_best_scores.csv'), index=False)\n",
    "\n",
    "    print('\nSaved full Decision Tree evaluation results to `decision_tree_full_results.csv`')\n",
    "    print('Saved best Decision Tree scores per dataset to `decision_tree_best_scores.csv`')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Internal Statistical Analysis of Decision Tree Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_DATASETS:\n",
    "    print('\nPerforming internal statistical analysis for Decision Tree hyperparameters on the top 12 datasets...')\n",
    "\n",
    "    # Create a unique identifier for each hyperparameter combination\n",
    "    full_dt_results_df['param_combo'] = full_dt_results_df.apply(\n",
    "        lambda row: f"crit={row['criterion']},depth={row['max_depth']}", axis=1\n",
    "    )\n",
    "\n",
    "    # Create a pivot table for Friedman test: datasets as rows, param_combo as columns, f1_cv_mean as values\n",
    "    pivot_dt_stats = full_dt_results_df.pivot_table(\n",
    "        index='dataset', \n",
    "        columns='param_combo', \n",
    "        values='f1_cv_mean'\n",
    "    )\n",
    "\n",
    "    print("Decision Tree F1-scores for different hyperparameter combinations on Top 12 Datasets:")\n",
    "    display(pivot_dt_stats)\n",
    "\n",
    "    # --- Friedman Test ---\n",
    "    stat, p_value = friedmanchisquare(*[pivot_dt_stats[col] for col in pivot_dt_stats.columns])\n",
    "\n",
    "    print(f'\nFriedman Test Results for Decision Tree Hyperparameters:')\n",
    "    print(f'  - Statistic: {stat:.4f}')\n",
    "    print(f'  - p-value: {p_value:.4f}')\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print('\n  Conclusion: There is a statistically significant difference between different Decision Tree hyperparameter combinations.')\n",
    "        \n",
    "        # --- Nemenyi Post-hoc Test ---\n",
    "        nemenyi_results = sp.posthoc_nemenyi_friedman(pivot_dt_stats.T)\n",
    "        \n",
    "        print('\nNemenyi Post-hoc Test Results (p-values for hyperparameter comparison):')\n",
    "        display(nemenyi_results)\n",
    "        \n",
    "        # --- Visualize Nemenyi Results ---\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sp.sign_plot(nemenyi_results, **{'style': 'default'})\n",
    "        plt.title('Nemenyi Test Critical Difference Plot for Decision Tree Hyperparameters')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print('\n  Conclusion: There is no statistically significant difference between different Decision Tree hyperparameter combinations.')\n",
    "else:\n",
    "    print('Cannot perform statistical analysis without BEST_DATASETS. Please run Notebook 02 first.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Overall Top Decision Tree Performing Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_DATASETS:\n",
    "    display(best_dt_df.sort_values(by='f1_cv_mean', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}